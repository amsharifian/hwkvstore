\subsection{Cache Policy}

In our system, the software decides which keys to place in the
accelerator. There are many constraints that this decision must satisfy.
First of all, the key itself should popular in the present and future
and not just based on past requests. Another more important
constaint is that the decision must be made \emph{quickly}. Otherwise,
cache handling will become the bottleneck. Lastly, the software can only
push keys to the accelerator every so often, since the accelerator cannot
serve requests while in write mode.

In order to find the hot keys and make quick decisions, we randomly sample
each key with probability $\frac{1}{8}$. This allows us, on average, to get
keys that appear frequently while ignoring many of the keys that only get
called a few times. Since the distribution of requests has a long tail, the
sampling mitigates any effects that this tail might have.

Since we would like to avoid setting the accelerator to write mode too often,
we batch key pushes. This means that we only push keys after we accumulate
$100$ distinct keys.

The accelerator stores keys as a two-way set-associative cache. Thus, there might
be a collision between the key being inserted and a key already set in the cache.
To determine whether to evict the previously set key or reject the new key,
the accelerator compares the count stored on the accelerator with the count
passed in through the reserve key instruction. After each batch insertion,
the software resets the hardware counts in order to avoid looking at outdated
information.
